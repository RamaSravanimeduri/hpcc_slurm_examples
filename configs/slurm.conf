# Sample config for SLURM Users Group
# Managment Policies
ClusterName=biocluster
ControlMachine=slurm
#ControlMachine=slurm,pelican
#ControlAddr=slurm
#BackupController=slurm2
SlurmctldPort=6817
SlurmdPort=6818
AuthType=auth/munge
CryptoType=crypto/munge
SlurmUser=root

# Location of logs and state info
#StateSaveLocation=/usr/local/var/tmp
#SlurmdSpoolDir=/usr/local/var/tmp/slurmd.%n.spool
#SlurmctldPidFile=/usr/local/var/run/slurmctld.pid
#SlurmdPidFile=/usr/local/var/run/slurmd.%n.pid
#SlurmctldLogFile=/usr/local/var/log/slurmctld.log
#SlurmdLogFile=/usr/local/var/log/slurmd.%n.log.%h

# Accounting
##JobAcctGatherType=jobacct_gather/linux
JobAcctGatherType=jobacct_gather/cgroup
JobAcctGatherFrequency=30
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageEnforce=limits
AccountingStorageLoc=slurm_acct_db
#AccountingStoragePort=8513
AccountingStoragePort=6819
AccountingStorageHost=slurm
AccountingStorageTRES=gres/gpu
# Job Completion Logs, plugin required
JobCompType=jobcomp/elasticsearch
JobCompLoc=http://kibana:9200

# Scheduling Policies
SchedulerParameters=bf_max_job_test=1000,bf_max_job_user=50
#SchedulerType=sched/builtin
SchedulerType=sched/backfill
FastSchedule=1

# JOB PREEMPTION
# Preempt by partition_prio/suspend seems to suspend jobs from the same user on the same node, oscillating which is running and which is suspended.
PreemptMode=Off
#PreemptMode=CHECKPOINT # Checkpoints or cancels
#PreemptMode=GANG,SUSPEND # Suspend job, but also allows resource sharing
PreemptType=preempt/none
#PreemptType=preempt/partition_prio

# JOB PRIORITY
FairShareDampeningFactor=1
PriorityType=priority/multifactor
PriorityDecayHalfLife=1-0
#PriorityCalcPeriod=
#PriorityFavorSmall=
PriorityMaxAge=7-0
#PriorityUsageResetPeriod=
PriorityWeightAge=10000
PriorityWeightFairshare=20000000
PriorityWeightJobSize=0
PriorityWeightPartition=100000000
PriorityWeightQOS=0
#PriorityFlags=ACCRUE_ALWAYS,SMALL_RELATIVE_TO_TIME,FAIR_TREE,MAX_TRES

# This should email admins when a job is not killable
#UnkillableStepProgram=/path/to/email/function

## Task Plugin controls assignment (binding) of tasks to CPU
# None - All tasks on a node can use all cpus on the node.
# Cgroup - cgroup subsystem is used to contain job to allocated CPUs. Portable Hardware Locality ( hwloc) library used to bind tasks to CPUs.
# Affinity - Bind tasks with one of the following:
# 	Cpusets     use cpuset subsystem to contain cpus assigned to tasks.
# 	Sched       use sched_setaffinity to bind tasks to cpus .
# In addition, a binding unit may also be specified. It can be one of Sockets, Cores, Threads, None
# Both the are specified on the TaskPluginParam statement.
TaskPlugin=task/cgroup
ProctrackType=proctrack/cgroup

# Allocaton Policies
SelectType=select/cons_res # Allows multiples jobs to run on a single node (resource)
SelectTypeParameters=CR_Core_Memory
#SelectTypeParameters=CR_Core
#SelectTypeParameters=CR_Memory
DefMemPerCPU=1024
GresTypes=gpu
#GresTypes=gpu,mem,gmem,scratch
DefMemPerNode=1024
MaxJobCount=50000
MaxArraySize=2500
MaxStepCount=40000
#MaxTasksPerNode=64
MemLimitEnforce=yes
#TmpFs=/tmp
# Still need to add slurm_pam.so to proper location in order for this to work properly
#UsePAM=1
# Automatically return node to service if valid config from slurmd is registered
#	0
#	    A node will remain in the DOWN state until a system administrator explicitly changes its state (even if the slurmd daemon registers and resumes communications). 
#	1
#	    A DOWN node will become available for use upon registration with a valid configuration only if it was set DOWN due to being non-responsive. If the node was set DOWN for any other reason (low memory, unexpected reboot, etc.), its state will not automatically be changed. A node registers with a valid configuration if its memory, GRES, CPU count, etc. are equal to or greater than the values configured in slurm.conf. 
#	2
#	    A DOWN node will become available for use upon registration with a valid configuration. The node could have been set DOWN for any reason. A node registers with a valid configuration if its memory, GRES, CPU count, etc. are equal to or greater than the values configured in slurm.conf. (Disabled on Cray ALPS systems.) 
ReturnToService=1

# Node Definitions
#NodeName=DEFAULT Sockets=2 CoresPerSocket=4 ThreadsPerCore=2
NodeName=pigeon Sockets=2 CoresPerSocket=4 ThreadsPerCore=2
NodeName=pelican Sockets=2 CoresPerSocket=8 ThreadsPerCore=2
NodeName=globus Sockets=2 CoresPerSocket=8 ThreadsPerCore=2
NodeName=penguin CPUS=8 RealMemory=60000 Sockets=2 CoresPerSocket=4 ThreadsPerCore=1
NodeName=owl Sockets=4 CoresPerSocket=4 ThreadsPerCore=1
NodeName=charmander Sockets=2 CoresPerSocket=8 ThreadsPerCore=2
#NodeName=slurm Sockets=1 CoresPerSocket=1 ThreadsPerCore=1
# The AMD partition needs to use all 64 cores as physical cores. Try redefining the c nodes here then reboot slurmctld and all slurmd's
NodeName=c[01-48] CPUs=64 RealMemory=500000 State=UNKNOWN
#NodeName=c[01-48] CPUs=64 RealMemory=500000 Sockets=4 CoresPerSocket=8 ThreadsPerCore=2 State=UNKNOWN
NodeName=h[01-06] CPUs=32 RealMemory=1000000 Sockets=4 CoresPerSocket=8 ThreadsPerCore=1 State=UNKNOWN #Feature=HyperThread
NodeName=gpu[01-02] CPUs=32 RealMemory=115000 Sockets=2 CoresPerSocket=8 ThreadsPerCore=2 Gres=gpu:4 State=UNKNOWN
NodeName=gpu[03-04] CPUs=32 RealMemory=500000 Sockets=2 CoresPerSocket=8 ThreadsPerCore=2 Gres=gpu:8 State=UNKNOWN
NodeName=gpu05 CPUs=64 RealMemory=200000 Sockets=2 CoresPerSocket=16 ThreadsPerCore=2 Gres=gpu:2 State=UNKNOWN
NodeName=i[01-44] CPUs=64 RealMemory=450000 Sockets=2 CoresPerSocket=16 ThreadsPerCore=2 State=UNKNOWN #Feature=HyperThread
NodeName=i[45-50] CPUs=64 RealMemory=200000 Sockets=2 CoresPerSocket=16 ThreadsPerCore=2 State=UNKNOWN #Feature=HyperThread
NodeName=m02 CPUs=64 RealMemory=450000 Sockets=4 CoresPerSocket=8 ThreadsPerCore=2 State=UNKNOWN 
NodeName=m[04-05] CPUs=64 RealMemory=100000 Sockets=4 CoresPerSocket=8 ThreadsPerCore=2 State=UNKNOWN 

## Preempt Modes
# Off
# Cancel - preempted job is cancelled.
# Checkpoint - preempted job is checkpointed if possible, or cancelled.
# Gang - enables time slicing of jobs on the same resource.
# Requeue - job is requeued and restarted at the beginning (only for sbatch ).
# Suspend - job is suspended until the higher priority job ends (requires Gang).

## OverSubscribe (Shared) Option
# Controls the ability of the partition to execute more than one job on a resource (node, socket, core)
# EXCLUSIVE allocates entire node (overrides cons_res ability to allocate cores and sockets to multiple jobs) 
# NO sharing of any resource.
# YES all resources can be shared, unless user specifies - exclusive on srun | salloc | sbatch
# FORCE all resources can be shared and user cannot override. (Generally only recommended for BlueGene , although FORCE:1 means that users cannot use - exclusive, but resources allocated to a job will not be shared.)

## Custom partitions ##
# Debug Partition
PartitionName=debug AllowGroups=operations PriorityTier=1 AllowQos=normal Nodes=penguin \
Shared=NO OverSubscribe=NO DefaultTime=10080 MaxTime=INFINITE State=UP \
TRESBillingWeights="CPU=1.0,Mem=0.25G"

# Short Partition
PartitionName=short PriorityTier=1 QOS=short AllowQos=short Nodes=i[01-50] \
Shared=NO OverSubscribe=NO DefaultTime=10080 MaxTime=120 State=UP \
TRESBillingWeights="CPU=1.0,Mem=0.25G"

# Intel Partition
PartitionName=intel PriorityTier=1 AllowQos=normal Nodes=i[01-02,17-40] \
Shared=NO OverSubscribe=NO Default=YES DefaultTime=10080 MaxTime=30-00:00 State=UP \
TRESBillingWeights="CPU=1.0,Mem=0.25G"

# Batch Partition
PartitionName=batch PriorityTier=1 AllowQos=normal Nodes=c[01-48] \
Shared=NO OverSubscribe=NO DefaultTime=10080 MaxTime=30-00:00 State=UP \
TRESBillingWeights="CPU=1.0,Mem=0.25G"

# Highmem Partition
PartitionName=highmem PriorityTier=1 QOS=highmem AllowQos=highmem Nodes=h[01-06] \
Shared=NO OverSubscribe=NO DefaultTime=2880 MaxTime=30-00:00 State=UP \
TRESBillingWeights="CPU=1.0,Mem=0.25G"

# GPU Partition
PartitionName=gpu PriorityTier=1 QOS=gpu AllowQos=gpu Nodes=gpu[01-05] \
Shared=NO OverSubscribe=NO DefaultTime=2880 MaxTime=30-00:00 State=UP \
TRESBillingWeights="CPU=1.0,Mem=0.25G,GRES/gpu=2.0"

## Private partitions ##
# You can hide partitions with Hidden=Yes
# Stajich Partition
PartitionName=stajichlab AllowGroups=stajichlab PriorityJobFactor=1000 PriorityTier=2 Nodes=m02,m[04-05] \
Shared=NO OverSubscribe=NO DefaultTime=10080 MaxTime=30-00:00 State=UP \
TRESBillingWeights="CPU=0,Mem=0"

# Girke Partition
PartitionName=girkelab AllowGroups=girkelab PriorityJobFactor=1000 PriorityTier=2 Nodes=i[03-04] \
Shared=NO OverSubscribe=NO DefaultTime=10080 MaxTime=30-00:00 State=UP \
TRESBillingWeights="CPU=0,Mem=0"

# Wenxiu Ma Partition
PartitionName=wmalab AllowGroups=wmalab PriorityJobFactor=1000 PriorityTier=2 Nodes=i[05-08] \
Shared=NO OverSubscribe=NO DefaultTime=10080 MaxTime=30-00:00 State=UP \
TRESBillingWeights="CPU=0,Mem=0"

# Koenig Partition
PartitionName=koeniglab AllowGroups=koeniglab PriorityJobFactor=1000 PriorityTier=2 Nodes=i[09-12] \
Shared=NO OverSubscribe=NO DefaultTime=10080 MaxTime=30-00:00 State=UP \
TRESBillingWeights="CPU=0,Mem=0"

# Greaney Partition
PartitionName=greaneylab AllowGroups=greaneylab PriorityJobFactor=1000 PriorityTier=2 Nodes=i[13-16,41-44] \
Shared=NO OverSubscribe=NO DefaultTime=10080 MaxTime=30-00:00 State=UP \
TRESBillingWeights="CPU=0,Mem=0"

# Stats Partition
PartitionName=statsdept AllowGroups=statsdept PriorityJobFactor=1000 PriorityTier=2 Nodes=i[45-48] \
Shared=NO OverSubscribe=NO DefaultTime=10080 MaxTime=30-00:00 State=UP \
TRESBillingWeights="CPU=0,Mem=0"

# Yehua Li Partition
PartitionName=ylilab AllowGroups=ylilab PriorityJobFactor=1000 PriorityTier=2 Nodes=i[49-50] \
Shared=NO OverSubscribe=NO DefaultTime=10080 MaxTime=30-00:00 State=UP \
TRESBillingWeights="CPU=0,Mem=0"

# Licenses
Licenses=intel:1

# Topology
#TopologyPlugin=topology/none
# Define node to IB switch mapping
TopologyPlugin=topology/tree

# Define QOS Example, our QOS rules are defined in update_slurm.sh in the hpcc_dashboard repo
#sacctmgr add qos meremortal
#sacctmgr add qos vip Preempt=meremortal PreemptMode=cancel

# Include QOS in association definition
#sacctmgr add user Rod DefaultAccount=math qos=vip,normal DefaultQOS=normal

# SLURM Front end name, can be host name, or FQDN, or DEFAULT
# Disabled at compile time
#FrontendName=slurm

